<div align= "center">
    <h1> 🧩 ProactiveBench </h1>
</div>

# 概要

ProactiveBench は、プロアクティブエージェントを評価するためのベンチマークです。データセット、報酬モデル、および評価スクリプトが含まれています。
私たちのテストセットには、コーディング、ライティング、および日常生活の3つのカテゴリのイベントが含まれています。
現在、テストセットには`227`のイベントが含まれています。
報酬モデルはデータセットでトレーニングされ、テストセットで`0.918`のF1スコアを達成しています。
プロアクティブエージェントと報酬モデルのパフォーマンスを評価するためのすべてのスクリプトを提供します。

## 報酬モデルの評価

報酬モデルは、プロアクティブエージェントのパフォーマンスを評価するために使用されます。
ここから報酬モデルをダウンロードし（近日公開）、[VLLM](https://github.com/vllm-project/vllm)などのフレームワークを使用してOpenAIスタイルのAPIを提供することができます。

その後、スクリプト`reward_model_scoring.py`を変更してモデルのアドレスを設定し、次のコマンドを実行します。

```bash
python eval/reward_model_scoring.py
```

このプロセスの後、報酬モデルの最終スコアを取得できます。

## プロアクティブエージェントの評価

モデルのパフォーマンスを確認するには、`./eval/script.py`を変更してモデルをロードし（またはSDKを使用）、次のコマンドを実行します。

```bash
python eval/script.py
```

テストデータはモデルに送信され、すべてのトレースとエージェントの応答は`./eval/traces_new`フォルダに保存されます。
このプロセスの後、次のコマンドを実行できます。

```bash
# スクリプトを実行する前に、judge_agent_prediction.pyのアドレスを報酬モデルのアドレスに変更する必要があります。
sh eval/judge_result.sh
```

これにより、報酬モデルがエージェントからの応答が受け入れ可能かどうかを評価します。結果は`./eval/judged`フォルダに保存されます。

報酬モデルによって評価された後、次のコマンドを実行できます。

```bash
sh calculate.sh
```

最終的にモデルのスコアを取得します。
